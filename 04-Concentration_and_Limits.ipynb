{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd1467f",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2023/)    \n",
    "## 1MS041, 2023 \n",
    "&copy;2023 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6829082",
   "metadata": {},
   "source": [
    "## Concentration and limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737ef11",
   "metadata": {},
   "source": [
    "Consider an i.i.d. sequence of random variables $X_1,\\ldots,X_n$ each being Bernoulli($1/2$). Then the concept of concentration is telling us that\n",
    "\n",
    "$$\n",
    "    P\\left ( \\left | \\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}(X_i) \\right | > \\epsilon \\right )\n",
    "$$\n",
    "\n",
    "gets smaller as $n$ gets larger. For instance, using Chebychevs inequality we get\n",
    "\n",
    "$$\n",
    "    P\\left ( \\left | \\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}(X_i) \\right | > \\epsilon \\right ) \\leq \\frac{\\mathbb{V}\\left( \\frac{1}{n} \\sum_{i=1}^n X_i \\right )}{\\epsilon^2} = \\frac{\\mathbb{V}\\left( X_0 \\right )}{\\epsilon^2 n}\n",
    "$$\n",
    "\n",
    "We can see that this is at least true in the simulation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145cbff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f532067995547838152754a719c589c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n', max=50, min=1, step=5), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "from Utils import discrete_histogram\n",
    "import numpy as np\n",
    "@interact \n",
    "def concentration(n=IntSlider(1,1,50,5)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    X = np.random.randint(0,2,size=(n,10000))*2-1\n",
    "    means = np.mean(X,axis=0)\n",
    "    print(\"P(mean > mu + 0.3 ) = %.5f <= Chebychev %.5f\" % (np.mean(means > 0.3),1/(0.3**2*n)))\n",
    "    discrete_histogram(means,normed=True)\n",
    "    plt.xlim(-1,1)\n",
    "\n",
    "# 分析一下这句的语法：\n",
    "# print(\"P(mean > mu + 0.3 ) = %.5f <= Chebychev %.5f\" % (np.mean(means > 0.3), 1 / (0.3**2 * n)))\n",
    "# 1.P(mean > mu + 0.3 ) = %.5f <= Chebychev %.5f\" 是一个包含占位符 %f 的字符串。\n",
    "  # 这些占位符表示将来要在字符串中插入浮点数\n",
    "# 2.百分号格式化：\n",
    "  # %是字符串格式化的操作符，右边的元组 (np.mean(means > 0.3), \n",
    "  # 1 / (0.3**2 * n)) 包含了要插入占位符的值。\n",
    "  # %.5f 表示浮点数的格式，保留小数点后五位。\n",
    "# 3.插入值：\n",
    "  # np.mean(means > 0.3) 是一个布尔表达式，它计算均值大于0.3的概率。\n",
    "  # 这个值将插入到第一个 %f 的位置。\n",
    "  # 1 / (0.3**2 * n) 是 Chebyshev 不等式计算的上界，表示均值大于0.3的概率上限。\n",
    "  # 这个值将插入到第二个 %f 的位置\n",
    "# 4.打印结果：\n",
    "  # print() 函数用于将格式化的字符串打印到控制台。\n",
    "#整句要表达的意思，包括了两个部分：\n",
    "#实验结果：它计算了均值大于0.3的概率，\n",
    "    #具体地说，np.mean(means > 0.3) 计算了样本均值中有多少比例大于0.3的值，\n",
    "    #然后通过字符串格式化插入到消息中。\n",
    "#理论上的上界：它使用chebychev不等式计算了理论上的上界\n",
    "    #具体来说，1 / (0.3**2 * n) 表示通过 Chebyshev 不等式得到的上界，\n",
    "    #然后也通过字符串格式化插入到消息中\n",
    "#整个消息的格式是：\"P(mean > mu + 0.3 ) = 实验结果 <= Chebychev 理论上的上界\"，\n",
    "    #这个消息告诉用户两个信息：实验中观察到的均值大于0.3的概率，\n",
    "    #以及 Chebyshev 不等式提供的理论上的上界。\n",
    "    #这有助于比较实际观察到的概率和理论上的概率上限。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e66c4",
   "metadata": {},
   "source": [
    "Hoeffdings inequality on the other hand is sharper and gives a bound much smaller, namely that\n",
    "\n",
    "$$\n",
    "    P\\left ( \\left |\\frac{1}{n} \\sum_{i=1}^n X_i - \\mathbb{E}(X_i) \\right | > \\epsilon \\right ) \n",
    "    \\leq 2e^{-2n \\epsilon^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de57b40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdeeaf1a35f4d8cb204d2fdcf9b1808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n', max=50, min=1, step=5), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact \n",
    "def concentration(n=IntSlider(1,1,50,5)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    X = np.random.randint(0,2,size=(n,10000))\n",
    "    means = np.mean(X,axis=0)\n",
    "    print(\"P(mean > mu + 0.3 ) = %.5f <= Hoeffding %.5f\" % (np.mean(means > 0.5+0.3),np.exp(-2*n*0.3**2)))\n",
    "    discrete_histogram(means,normed=True)\n",
    "    plt.xlim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c07bb",
   "metadata": {},
   "source": [
    "This is much closer, and in fact very close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f084b",
   "metadata": {},
   "source": [
    "### Using concentration as a measure of confidence\n",
    "\n",
    "We can use concentration as a measure of confidence in the following way. Consider $X_1,\\ldots, X_n$ being i.i.d. sequence of Bernoulli($p$) for some unknown $p$. From the concept of concentration, we would expect that if we have many observations ($n$ large) we could use the empirical mean of the observations as a guess, but note that there is some variability as we saw in the above simulations. So what do we do? We use the concentration inequality to get information how far we can deviate from $p$ in the following way\n",
    "\n",
    "$$\n",
    "    P(\\bar X_n - \\mathbb{E}(\\bar X_n) \\geq \\epsilon) \\leq e^{-2n\\epsilon^2}\n",
    "$$\n",
    "\n",
    "Since $\\mathbb{E}(\\bar X_n) = p$, rearrange and get\n",
    "\n",
    "$$\n",
    "    P(p \\leq \\bar X_n - \\epsilon) \\leq e^{-2n\\epsilon^2}\n",
    "$$\n",
    "\n",
    "The complementary event thus satisfies\n",
    "\n",
    "$$\n",
    "    P(\\bar X_n - \\epsilon < p) \\geq 1-e^{-2n\\epsilon^2}\n",
    "$$\n",
    "\n",
    "We can do the same for the other side (see lecture notes) and we get\n",
    "\n",
    "$$\n",
    "    P(\\bar X_n - \\epsilon < p < \\bar X_n + \\epsilon) \\geq 1-2 e^{-2n\\epsilon^2}.\n",
    "$$\n",
    "\n",
    "If you where now asked to estimate $p$ using $n$ observations and give an interval where you with at least 95% confidence can say contains $p$, then you need to choose $\\epsilon > 0$ such that\n",
    "\n",
    "$$\n",
    "    1-2 e^{-2n\\epsilon^2} \\geq 0.95.\n",
    "$$\n",
    "\n",
    "Smaller $\\epsilon$ gives smaller intervals, so lets choose to have the smallest possible $\\epsilon$ while still obaying the inequality above, i.e. we choose $\\epsilon$ to solve\n",
    "\n",
    "$$\n",
    "    1-2 e^{-2n\\epsilon^2} = 0.95.\n",
    "$$\n",
    "\n",
    "Rearranging we and taking log and then square root we obtain\n",
    "\n",
    "$$\n",
    "    \\epsilon = \\sqrt{-\\frac{1}{2n}\\ln\\left(\\frac{1-0.95}{2}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69735ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b149023673ce43b5b23a3bdb0adad78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='n', max=500, min=50, step=50), FloatSlider(value=0.5, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FloatSlider\n",
    "@interact #引入交互式函数\n",
    "def concentration(n=IntSlider(value=1,min=50,max=500,step=50),p=FloatSlider(value=0.5, min=0,max=1,step=0.1)):\n",
    "    X = np.random.binomial(1,p,size=(n))\n",
    "    means = np.mean(X,axis=0)\n",
    "    epsilon = np.sqrt(-1/(2*n)*np.log((1-0.95)/2))\n",
    "    #print(\"95%% confidence interval [%.2f, %.2f] for p=%.2f n=%d\" % (means-epsilon,means+epsilon,p,n))\n",
    "    print(f\"95% confidence interval [{means-epsilon:.2f}, {means+epsilon:.2f}] for p={p:.2f} n={n}\")\n",
    "    #用f-string语法更为简单清晰\n",
    "#在 f-string 中，花括号 {} 用来标记将要插入的变量或表达式。\n",
    "#在花括号中，可以使用变量、表达式或函数等。\n",
    "#在花括号中使用冒号 : 后，可以指定格式化的方式。例如，.2f 表示保留两位小数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83238cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc1c11fe09d40d9ab473031beb6f165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='n', max=5000, min=50, step=50), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FloatSlider\n",
    "@interact \n",
    "def concentration_300(n=IntSlider(value=1,min=50,max=5000,step=50)):\n",
    "    X = np.minimum(np.abs(70+6*np.random.normal(size=(n))),300)\n",
    "    #这一句的语法结构如下：\n",
    "    #np.random.normal(size=(n)) 生成一个大小为 n 的数组，\n",
    "    #其中的元素是从标准正态分布（均值为0，标准差为1）中抽取的随机数。\n",
    "    #将上述随机数乘以6，然后加上70，得到一个服从均值为70、标准差为6的正态分布的数组\n",
    "    #使用 np.abs 计算上述数组中每个元素的绝对值，确保所有的值都为非负数。\n",
    "    #使用 np.minimum 函数将上述数组中的每个元素与300比较，保留较小的那个值。\n",
    "    \n",
    "    means = np.mean(X,axis=0)\n",
    "    epsilon = np.sqrt(-1/(2*n/(300**2))*np.log((1-0.95)/2))\n",
    "    print(\"95%% confidence interval [%.2f, %.2f] for n=%d\" % (means-epsilon,means+epsilon,n))\n",
    "    print(f\"95% confidence interval [{means-epsilon:.2f},{means+epsilon:.2f}] for n={n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f61f15",
   "metadata": {},
   "source": [
    "These things are super useful. Lets go back to our sms spam/ham problem and see what we can say there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f681b9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 0), ('Ok lar... Joking wif u oni...', 0)]\n"
     ]
    }
   ],
   "source": [
    "from Utils import load_sms \n",
    "sms_data = load_sms()\n",
    "sms_data[:2]   #这里代表的是选取了sms_data的前两行\n",
    "\n",
    "print(sms_data[:2])  \n",
    "\n",
    "#若要取sms_data的前两列，语法为：sms_data[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c6cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_words=set(['free','prize'])   #创立一个集合，包含两个string单词\n",
    "TF10 = {True: 1, False: 0}   #定义TF10字典，把布尔值映射为两个数字\n",
    "Z_obs = [TF10[not interesting_words.isdisjoint([word.lower() for word in line[0].split(' ')])] for line in sms_data]\n",
    "#对sms_data里的每行遍历，把每行的首个单词分离出来，并转换成小写\n",
    "#看看这个分离出来的单词是否满足 不和interestingword交集为空，即和interestingword有交集\n",
    "#按照TF10的映射，若有交集，布尔值为1，若无交集，布尔值为0，\n",
    "#这些值一起组成一个列表，命名为Z_obs\n",
    "#print(Z_obs)\n",
    "\n",
    "#我感觉上面的语法有点拗口，双重否定。如果换一种语法：\n",
    "M=[bool(set(interesting_words) & set([word.lower() for word in line[0].split(' ')]))for line in sms_data]\n",
    "M_obs=[TF10[x] for x in M]\n",
    "\n",
    "Y_obs = [y for x,y in sms_data]  # Y_obs 是 sms_data 里面的标签值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cae79e",
   "metadata": {},
   "source": [
    "Recall that we computed the mean of Y_obs, which is the mean spam number. This is a Bernoulli random variable with unknown $p$, so we can use our methods above to compute a confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94e3af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_bernoulli(n,alpha):   #定义了一个epsilon_bernoulli函数，接受n和alpha两个参数\n",
    "    return np.sqrt(-1/(2*n)*np.log((alpha)/2))   #返回的是 Bernoulli 分布的置信区间半径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3662b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.116,0.152]\n"
     ]
    }
   ],
   "source": [
    "epsilon = epsilon_bernoulli(len(Y_obs),0.05)   #n=len(Y_obs)，alpha=0.05，求epsilon\n",
    "mean_Y_obs = np.mean(Y_obs)\n",
    "print(\"[%.3f,%.3f]\" % (mean_Y_obs-epsilon,mean_Y_obs+epsilon))   #%.3f 代表字符串保留三位小数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331cfa99",
   "metadata": {},
   "source": [
    "From this we get that we have provided a prediction as to what is the true probability of getting a spam email.\n",
    "\n",
    "Lets take a look at the conditional probability. Recall that\n",
    "\n",
    "$$\n",
    "    P(Y = 1 \\mid Z = 1) = \\frac{P(Y = 1 \\text{ and } Z = 1)}{P(Z = 1)}\n",
    "$$\n",
    "\n",
    "but this requires you to estimate both things on the right, and give a region for both and finally figure out an interval for the ratio. But, there is an easier way, which sometimes works better. Namely, to look at the random variable $Y \\mid (Z=1)$ which we do by filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4acc244",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mid_Z1 = [y for z,y in zip(Z_obs,Y_obs) if z == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65cfed",
   "metadata": {},
   "source": [
    "Now for this you have a certain number of observations and we can use this instead as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52de97ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.726,0.898]\n"
     ]
    }
   ],
   "source": [
    "epsilon = epsilon_bernoulli(len(Y_mid_Z1),0.05)\n",
    "mean_Y_obs = np.mean(Y_mid_Z1)\n",
    "print(\"[%.3f,%.3f]\" % (mean_Y_obs-epsilon,mean_Y_obs+epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b51f6",
   "metadata": {},
   "source": [
    "## Being clever with Bennetts inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59388763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bennett_epsilon(n,b,sigma,alpha):\n",
    "    import scipy.optimize as so\n",
    "    h = lambda u: (1+u)*np.log(1+u)-u\n",
    "    f = lambda epsilon: np.exp(-n*sigma**2/b**2*h(b*epsilon/sigma**2))-alpha/2\n",
    "    ans = so.fsolve(f,0.002)\n",
    "    epsilon = np.abs(ans[0])\n",
    "    print(\"Numerical error\", f(epsilon))\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0267b9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical error -4.5102810375396984e-17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.457174445041813"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bennett_epsilon(50,300,20,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a25f7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ba018c39c84030a4d8edfa444ee8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='n', max=5000, min=50, step=50), FloatSlider(value=10.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FloatSlider\n",
    "@interact \n",
    "def concentration_300(n=IntSlider(value=1,min=50,max=5000,step=50),sigma=FloatSlider(value=10, min=1,max=300,step=1)):\n",
    "    X = np.minimum(np.abs(70+6*np.random.normal(size=(n))),300)\n",
    "    means = np.mean(X,axis=0)\n",
    "    epsilon = np.sqrt(-1/(2*n/(300**2))*np.log((1-0.95)/2))\n",
    "    epsilon2 = bennett_epsilon(n,b=300,sigma=sigma,alpha=0.05)\n",
    "    print(\"95%% confidence interval rough [%.2f, %.2f] for n=%d\" % (means-epsilon,means+epsilon,n))\n",
    "    print(\"95%% confidence interval Bennett [%.2f, %.2f] for n=%d\" % (means-epsilon2,means+epsilon2,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11663257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276ac35-8d50-4a10-aae6-d9fc556297b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "lx_course_instance": "2023",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
